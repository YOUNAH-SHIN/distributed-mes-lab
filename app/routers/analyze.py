import os
import re
from typing import List, Dict, Any, Optional, Set, Tuple

from datetime import datetime, timedelta  # âœ… timezone ì œê±°

from fastapi import APIRouter, HTTPException, Query, Depends
from sqlalchemy.orm import Session
from sqlalchemy import text

from app.db import get_db  # dashboard.py ì™€ ë™ì¼í•œ DB ì„¸ì…˜ ì‚¬ìš©

print(">>> loaded routers.analyze (MySQL version):", __file__, flush=True)

# /api/analyze ë¡œ ë¶„ë¦¬
router = APIRouter(prefix="/api/analyze", tags=["analyze"])

# ---- Debug toggle ----
DEBUG = os.getenv("ANALYZE_DEBUG", "0").lower() in ("1", "true", "yes", "on")


def dlog(*args: Any) -> None:
    if DEBUG:
        print(*args, flush=True)


# ---- MySQL env ----
DEVICE_TABLE = os.getenv("DEVICE_KPI_TABLE", "device_kpi")


def _validate_workcell(workcell: str) -> str:
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", workcell or ""):
        raise HTTPException(status_code=400, detail="Invalid workcell")
    return workcell


# ---------------- Range â†’ window ë§¤í•‘ ----------------
def _window_for_range(range_str: str) -> timedelta:
    """
    range ë¬¸ìžì—´ì„ ì‹¤ì œ ì‹œê°„ window ë¡œ ë³€í™˜.
    ì§€ì› ê°’:
      - "30m"  â†’ ìµœê·¼ 30ë¶„
      - "1day" â†’ ìµœê·¼ 1ì¼
      - "7day" â†’ ìµœê·¼ 7ì¼
    ê·¸ ì™¸ ê°’ì€ ê¸°ë³¸ 30ë¶„ìœ¼ë¡œ ì²˜ë¦¬
    """
    key = (range_str or "").strip().lower()

    if key == "30m":
        return timedelta(minutes=30)
    if key == "1day":
        return timedelta(days=1)
    if key == "7day":
        return timedelta(days=7)

    # ì•Œ ìˆ˜ ì—†ëŠ” ê°’ì´ë©´ ì•ˆì „í•˜ê²Œ 30ë¶„
    return timedelta(minutes=30)


# ---------------- DB helpers (SQLAlchemy + text) ----------------
def _fetch_all(db: Session, sql: str, params: Dict[str, Any]) -> List[Dict[str, Any]]:
    dlog("[ANALYZE SQL ALL]\n", sql, "\nparams:", params)
    rows = db.execute(text(sql), params).mappings().all()
    return [dict(r) for r in rows]


# ------------------------------------------------------------------------
# GET /api/analyze/device_types  â†’ {"types":[...]}
#   - MySQL device_kpi ì—ì„œ DISTINCT device_type
# ------------------------------------------------------------------------
@router.get("/device_types")
def list_device_types(
    workcell: str = Query(..., description="ì›Œí¬ì…€ ID (ì˜ˆ: A1)"),
    db: Session = Depends(get_db),
) -> Dict[str, Any]:
    workcell = _validate_workcell(workcell)

    sql = f"""
        SELECT DISTINCT device_type
        FROM {DEVICE_TABLE}
        WHERE workcell = :workcell
          AND device_type IS NOT NULL
          AND device_type <> ''
        ORDER BY device_type
    """
    rows = _fetch_all(db, sql, {"workcell": workcell})
    types: List[str] = [str(r["device_type"]) for r in rows if r.get("device_type")]

    dlog("[/api/analyze/device_types] count:", len(types), "types:", types)
    return {
        "types": types,
        "_source": "mysql",
        "_table": DEVICE_TABLE,
        "_workcell": workcell,
    }


# ------------------------------------------------------------------------
# GET /api/analyze/device_names  â†’ {"devices":[...]}
#   - ì˜µì…˜: device_type ë¡œ í•„í„°
# ------------------------------------------------------------------------
@router.get("/device_names")
def list_device_names(
    workcell: str = Query(..., description="ì›Œí¬ì…€ ID (ì˜ˆ: A1)"),
    device_type: Optional[str] = Query(
        None,
        description="í•„í„°: íŠ¹ì • device_type ë§Œ",
    ),
    db: Session = Depends(get_db),
) -> Dict[str, Any]:
    workcell = _validate_workcell(workcell)

    base_sql = f"""
        SELECT DISTINCT device_name
        FROM {DEVICE_TABLE}
        WHERE workcell = :workcell
          AND device_name IS NOT NULL
          AND device_name <> ''
    """
    params: Dict[str, Any] = {"workcell": workcell}

    if device_type:
        base_sql += " AND device_type = :device_type"
        params["device_type"] = device_type

    base_sql += " ORDER BY device_name"

    rows = _fetch_all(db, base_sql, params)
    names: List[str] = [str(r["device_name"]) for r in rows if r.get("device_name")]

    dlog(
        "[/api/analyze/device_names] count:",
        len(names),
        "workcell:",
        workcell,
        "device_type:",
        device_type or "(none)",
        "names:",
        names,
    )

    return {
        "devices": names,
        "_source": "mysql",
        "_table": DEVICE_TABLE,
        "_workcell": workcell,
        "_device_type": device_type,
    }


# ------------------------------------------------------------------------
# GET /api/analyze/devices
#   â†’ {"devices":[{"device_type": "...", "device_name": "..."}, ...]}
#   - type / name ë‘˜ ë‹¤ í•œ ë²ˆì— ë°›ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©
# ------------------------------------------------------------------------
@router.get("/devices")
def list_devices(
    workcell: str = Query(..., description="ì›Œí¬ì…€ ID (ì˜ˆ: A1)"),
    db: Session = Depends(get_db),
) -> Dict[str, Any]:
    workcell = _validate_workcell(workcell)

    sql = f"""
        SELECT DISTINCT device_type, device_name
        FROM {DEVICE_TABLE}
        WHERE workcell = :workcell
          AND device_name IS NOT NULL
          AND device_name <> ''
          AND device_type IS NOT NULL
          AND device_type <> ''
        ORDER BY device_type, device_name
    """
    rows = _fetch_all(db, sql, {"workcell": workcell})

    devices: List[Dict[str, str]] = []
    seen: Set[Tuple[str, str]] = set()

    for r in rows:
        t = str(r.get("device_type") or "")
        n = str(r.get("device_name") or "")
        if not t or not n:
            continue

        key = (t, n)
        if key in seen:
            continue
        seen.add(key)

        devices.append({"device_type": t, "device_name": n})

    dlog("[/api/analyze/devices] count:", len(devices), "devices:", devices)

    return {
        "devices": devices,
        "_source": "mysql",
        "_table": DEVICE_TABLE,
        "_workcell": workcell,
    }


# ------------------------------------------------------------------------
# GET /api/analyze/timeseries
# ------------------------------------------------------------------------
@router.get("/timeseries")
def timeseries(
    workcell: str = Query(..., description="ì›Œí¬ì…€ ID (ì˜ˆ: A1)"),
    metric: str = Query(
        "defect_rate",
        description='metric: "defect_rate" ë˜ëŠ” "cycle" (cycle time)',
    ),
    range: str = Query(
        "30m",
        alias="range",
        description='"30m", "1day", "7day" ì¤‘ í•˜ë‚˜',
    ),
    device_types: Optional[str] = Query(
        None,
        description="ì‰¼í‘œë¡œ êµ¬ë¶„ëœ device_type ëª©ë¡ (ì˜ˆ: Conveyor,Robot)",
    ),
    device_names: Optional[str] = Query(
        None,
        description="ì‰¼í‘œë¡œ êµ¬ë¶„ëœ device_name ëª©ë¡ (ì˜ˆ: X_Robot,Y_Robot)",
    ),
    db: Session = Depends(get_db),
) -> Dict[str, Any]:
    import traceback  # ë‚´ë¶€ ë””ë²„ê¹…ìš©

    try:
        workcell = _validate_workcell(workcell)

        metric_norm = metric.strip().lower()
        if metric_norm not in ("defect_rate", "cycle"):
            raise HTTPException(
                status_code=400,
                detail="metric must be 'defect_rate' or 'cycle'",
            )

        # ----- 1) ì•µì»¤ ì‹œê°„: ì´ ì›Œí¬ì…€ì˜ ìµœì‹  time (DB ê¸°ì¤€) -----
        anchor_sql = f"""
            SELECT MAX(time) AS max_time
            FROM {DEVICE_TABLE}
            WHERE workcell = :workcell
        """
        anchor_rows = _fetch_all(db, anchor_sql, {"workcell": workcell})
        anchor_time = (
            anchor_rows[0]["max_time"]
            if anchor_rows and anchor_rows[0].get("max_time") is not None
            else None
        )

        if anchor_time is None:
            dlog("[/api/analyze/timeseries] no data for workcell:", workcell)
            return {
                "workcell": workcell,
                "metric": metric_norm,
                "range": range,
                "series": [],
                "_source": "mysql",
                "_table": DEVICE_TABLE,
                "anchor_time": None,
                "t_from": None,
                "t_to": None,
            }

        if not isinstance(anchor_time, datetime):
            try:
                anchor_time = datetime.fromisoformat(str(anchor_time))
            except Exception:
                dlog("[/api/analyze/timeseries] anchor_time not datetime:", anchor_time)

        # ----- 2) ì‹œê°„ êµ¬ê°„: ì•µì»¤ ê¸°ì¤€, range ì— ë”°ë¼ ë™ì  -----
        window = _window_for_range(range)
        t_to = anchor_time
        t_from = anchor_time - window

        dlog(
            "[/api/analyze/timeseries] anchor_time:",
            anchor_time,
            "window:",
            window,
            "t_from:",
            t_from,
            "t_to:",
            t_to,
            "range_param:",
            range,
        )

        # ----- 3) í•„í„° íŒŒë¼ë¯¸í„° íŒŒì‹± (ì‰¼í‘œ êµ¬ë¶„) -----
        sel_types: Optional[List[str]] = None
        sel_names: Optional[List[str]] = None

        if device_types:
            sel_types = [s.strip() for s in device_types.split(",") if s.strip()]
        if device_names:
            sel_names = [s.strip() for s in device_names.split(",") if s.strip()]

        dlog(
            "[/api/analyze/timeseries] filters:",
            "types =", sel_types or [],
            "names =", sel_names or [],
        )

        # âœ… íƒ€ìž…ì´ í•˜ë‚˜ë¼ë„ ì„ íƒë˜ë©´ type ì‹œë¦¬ì¦ˆëŠ” í•­ìƒ ë§Œë“ ë‹¤
        want_type_series = bool(sel_types)
        # âœ… ë””ë°”ì´ìŠ¤ëŠ” name ì„ íƒ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´, sel_namesê°€ ì—†ìœ¼ë©´ ì „ì²´
        want_device_series = True

        # ----- 4) SQL êµ¬ì„± -----
        # ðŸ”¹ device_kpi ìŠ¤í‚¤ë§ˆ ê¸°ì¤€:
        #   ct           â†’ cycle_time_s
        #   `count`      â†’ total_count
        #   defect_count â†’ defect_count
        sql = f"""
            SELECT
                time,
                device_type,
                device_name,
                ct AS cycle_time_s,
                `count` AS total_count,
                defect_count
            FROM {DEVICE_TABLE}
            WHERE workcell = :workcell
              AND time >= :t_from
              AND time <= :t_to
              AND device_type IS NOT NULL
              AND device_type <> ''
              AND device_name IS NOT NULL
              AND device_name <> ''
        """
        params: Dict[str, Any] = {
            "workcell": workcell,
            "t_from": t_from,
            "t_to": t_to,
        }

        # IN (...) placeholder í™•ìž¥
        if sel_types:
            type_ph = []
            for i, tval in enumerate(sel_types):
                key = f"type_{i}"
                type_ph.append(f":{key}")
                params[key] = tval
            sql += f" AND device_type IN ({', '.join(type_ph)})"

        if sel_names:
            name_ph = []
            for i, nval in enumerate(sel_names):
                key = f"name_{i}"
                name_ph.append(f":{key}")
                params[key] = nval
            sql += f" AND device_name IN ({', '.join(name_ph)})"

        sql += " ORDER BY time ASC"

        rows = _fetch_all(db, sql, params)
        dlog("[/api/analyze/timeseries] raw row count:", len(rows))

        # ìž¥ë¹„ë³„ ë¡œìš° ì¹´ìš´íŠ¸ ë¡œê·¸
        per_key: Dict[str, int] = {}
        for r in rows:
            dtp = str(r.get("device_type") or "").strip()
            dnm = str(r.get("device_name") or "").strip()
            key = f"{dtp}::{dnm}"
            per_key[key] = per_key.get(key, 0) + 1
        dlog("[/api/analyze/timeseries] row count by device:", per_key)

        # ----- 5) ì‹œë¦¬ì¦ˆ ì§‘ê³„ êµ¬ì¡° -----
        series_map: Dict[str, Dict[str, Any]] = {}

        def _ensure_series(
            key: str,
            kind: str,
            device_type: str,
            device_name: Optional[str],
        ) -> Dict[str, Any]:
            if key not in series_map:
                series_map[key] = {
                    "key": key,
                    "kind": kind,  # "type" or "device"
                    "device_type": device_type,
                    "device_name": device_name,
                    "time": [],
                    "values": [],
                    "_agg": {},  # t_iso -> {sum, cnt, sum_count, sum_def}
                }
            return series_map[key]

        for r in rows:
            t_raw = r.get("time")
            if t_raw is None:
                continue

            if isinstance(t_raw, datetime):
                t_iso = t_raw.isoformat()
            else:
                t_iso = str(t_raw)

            dtp = str(r.get("device_type") or "").strip()
            dnm = str(r.get("device_name") or "").strip()
            if not dtp or not dnm:
                continue

            cycle_val = r.get("cycle_time_s")
            total_cnt = r.get("total_count")
            defect_cnt = r.get("defect_count")

            # ----- device_name ì‹œë¦¬ì¦ˆ -----
            if want_device_series:
                if (not sel_names) or (dnm in sel_names):
                    dev_key = f"device:{dnm}"
                    dev_ser = _ensure_series(dev_key, "device", dtp, dnm)
                    ag = dev_ser["_agg"].setdefault(
                        t_iso,
                        {"sum": 0.0, "cnt": 0, "sum_count": 0.0, "sum_def": 0.0},
                    )

                    if metric_norm == "cycle":
                        if cycle_val is not None:
                            ag["sum"] += float(cycle_val)
                            ag["cnt"] += 1

                    elif metric_norm == "defect_rate":
                        if total_cnt is not None and defect_cnt is not None:
                            try:
                                c = float(total_cnt)
                                d = float(defect_cnt)
                            except (TypeError, ValueError):
                                c = 0.0
                                d = 0.0
                            ag["sum_count"] += c
                            ag["sum_def"] += d

            # ----- device_type ì‹œë¦¬ì¦ˆ (ìƒìœ„ ì§‘ê³„) -----
            if want_type_series:
                if (not sel_types) or (dtp in sel_types):
                    type_key = f"type:{dtp}"
                    type_ser = _ensure_series(type_key, "type", dtp, None)
                    ag_t = type_ser["_agg"].setdefault(
                        t_iso,
                        {"sum": 0.0, "cnt": 0, "sum_count": 0.0, "sum_def": 0.0},
                    )

                    if metric_norm == "cycle":
                        if cycle_val is not None:
                            ag_t["sum"] += float(cycle_val)
                            ag_t["cnt"] += 1

                    elif metric_norm == "defect_rate":
                        if total_cnt is not None and defect_cnt is not None:
                            try:
                                c = float(total_cnt)
                                d = float(defect_cnt)
                            except (TypeError, ValueError):
                                c = 0.0
                                d = 0.0
                            ag_t["sum_count"] += c
                            ag_t["sum_def"] += d

        # ----- 6) agg â†’ time / values ë°°ì—´ë¡œ ë³€í™˜ -----
        result_series: List[Dict[str, Any]] = []

        for key, ser in series_map.items():
            agg = ser.pop("_agg", {})
            keys_sorted = sorted(agg.keys())

            time_list: List[str] = []
            val_list: List[Optional[float]] = []

            for t_iso in keys_sorted:
                info = agg[t_iso]
                if metric_norm == "cycle":
                    if info["cnt"] > 0:
                        # í•©ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì›í•˜ë©´ í‰ê·  info["sum"]/info["cnt"] ë¡œ ë³€ê²½ ê°€ëŠ¥)
                        value = info["sum"]
                    else:
                        value = None
                elif metric_norm == "defect_rate":
                    if info["sum_count"] > 0:
                        # good ratio %
                        good_ratio_pct = (
                            (info["sum_count"] - info["sum_def"])
                            * 100.0
                            / info["sum_count"]
                        )
                        value = good_ratio_pct
                    else:
                        value = None
                else:
                    value = None

                time_list.append(t_iso)
                val_list.append(value)

            ser["time"] = time_list
            ser["values"] = val_list
            result_series.append(ser)

            dlog(
                "[/api/analyze/timeseries] series built:",
                key,
                "points:",
                len(time_list),
            )

        dlog(
            "[/api/analyze/timeseries] series count:",
            len(result_series),
            "workcell:",
            workcell,
            "metric:",
            metric_norm,
            "range(param):",
            range,
        )

        return {
            "workcell": workcell,
            "metric": metric_norm,
            "range": range,
            "series": result_series,
            "_source": "mysql",
            "_table": DEVICE_TABLE,
            "anchor_time": anchor_time.isoformat()
            if isinstance(anchor_time, datetime)
            else str(anchor_time),
            "t_from": t_from.isoformat()
            if isinstance(t_from, datetime)
            else str(t_from),
            "t_to": t_to.isoformat()
            if isinstance(t_to, datetime)
            else str(t_to),
        }

    except HTTPException:
        raise
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"timeseries internal error: {e}",
        )
